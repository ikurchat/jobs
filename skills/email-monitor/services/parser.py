"""–ü–∞—Ä—Å–µ—Ä –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –ø–∏—Å–µ–º.

–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞,
–≥–æ—Ç–æ–≤–∏—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è owner'—É.
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–æ—Ä–≤–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∏—Å—å–º–∞ –∏ HTML-–∫–æ–Ω—Ç–µ–Ω—Ç –°–≠–î.

CLI: python -m services.parser parse --email email.json
     python -m services.parser summary --email email.json
     python -m services.parser extract_tasks --email email.json
"""

import argparse
import json
import re
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config.settings import load_config, output_json, output_error


def _strip_html(html: str) -> str:
    """–û—á–∏—â–∞–µ—Ç HTML –¥–æ —á–∏—Ç–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    if not html:
        return ""
    # –ó–∞–º–µ–Ω—è–µ–º <br>, <p>, <tr> –Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    text = re.sub(r"<br\s*/?>", "\n", html, flags=re.IGNORECASE)
    text = re.sub(r"</(?:p|tr|div|li|h[1-6])>", "\n", text, flags=re.IGNORECASE)
    text = re.sub(r"<(?:td|th)[^>]*>", " | ", text, flags=re.IGNORECASE)
    # –£–¥–∞–ª—è–µ–º style –∏ script –±–ª–æ–∫–∏ —Ü–µ–ª–∏–∫–æ–º
    text = re.sub(r"<style[^>]*>.*?</style>", "", text, flags=re.IGNORECASE | re.DOTALL)
    text = re.sub(r"<script[^>]*>.*?</script>", "", text, flags=re.IGNORECASE | re.DOTALL)
    # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–µ–≥–∏
    text = re.sub(r"<[^>]+>", "", text)
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º HTML-—Å—É—â–Ω–æ—Å—Ç–∏
    text = text.replace("&nbsp;", " ").replace("&amp;", "&")
    text = text.replace("&lt;", "<").replace("&gt;", ">")
    text = text.replace("&quot;", '"').replace("&#39;", "'")
    # –ß–∏—Å—Ç–∏–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n\s*\n+", "\n\n", text)
    return text.strip()


def _parse_forwarded_headers(text: str) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–æ—Ä–≤–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∏—Å—å–º–∞."""
    result = {}
    # –ü–∞—Ç—Ç–µ—Ä–Ω: "–û—Ç: address" –∏–ª–∏ "From: address"
    m = re.search(r"–û—Ç:\s*(\S+@\S+)", text)
    if not m:
        m = re.search(r"From:\s*(\S+@\S+)", text)
    if m:
        result["original_sender"] = m.group(1).strip()

    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è —Ç–µ–º–∞
    m = re.search(r"–¢–µ–º–∞:\s*(.+?)(?:\n|$)", text)
    if not m:
        m = re.search(r"Subject:\s*(.+?)(?:\n|$)", text)
    if m:
        result["original_subject"] = m.group(1).strip()

    # –ü–∞–ø–∫–∞ —Ñ–æ—Ä–≤–∞—Ä–¥–µ—Ä–∞
    m = re.search(r"–ü–∞–ø–∫–∞:\s*(\S+)", text)
    if m:
        result["folder"] = m.group(1).strip()

    # –î–∞—Ç–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
    m = re.search(r"–î–∞—Ç–∞:\s*(\S+\s+\S+)", text)
    if m:
        result["original_date"] = m.group(1).strip()

    return result


def _extract_sed_table_data(html: str) -> dict:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ HTML-—Ç–∞–±–ª–∏—Ü –°–≠–î (–†–°–ö–¶-—Ñ–æ—Ä–º–∞—Ç)."""
    result = {
        "document_type": "",
        "document_number": "",
        "resolution_author": "",
        "resolution_text": "",
        "deadline": "",
        "executor": "",
        "controller": "",
        "status": "",
    }

    clean = _strip_html(html)

    # –ò—â–µ–º –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ —Ç–µ–º–µ –∏–ª–∏ —Ç–µ–ª–µ
    m = re.search(r"(?:‚Ññ|–†–µ–≥\.?\s*‚Ññ)\s*([\w\-/]+)", clean)
    if m:
        result["document_number"] = m.group(1)

    # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞ —Ä–µ–∑–æ–ª—é—Ü–∏–∏
    m = re.search(r"(?:–ê–≤—Ç–æ—Ä —Ä–µ–∑–æ–ª—é—Ü–∏–∏|–†–µ–∑–æ–ª—é—Ü–∏—è)\s*[:\|]\s*(.+?)(?:\n|\|)", clean)
    if m:
        result["resolution_author"] = m.group(1).strip()

    # –ò—â–µ–º —Ç–µ–∫—Å—Ç —Ä–µ–∑–æ–ª—é—Ü–∏–∏
    m = re.search(r"(?:–¢–µ–∫—Å—Ç —Ä–µ–∑–æ–ª—é—Ü–∏–∏|–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ)\s*[:\|]\s*(.+?)(?:\n\n|\|)", clean, re.DOTALL)
    if m:
        result["resolution_text"] = m.group(1).strip()[:300]

    # –ò—â–µ–º —Å—Ä–æ–∫
    m = re.search(r"(?:–ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π —Å—Ä–æ–∫|–°—Ä–æ–∫|–ò—Å–ø–æ–ª–Ω–∏—Ç—å –¥–æ)\s*[:\|]\s*(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})", clean)
    if m:
        result["deadline"] = m.group(1)

    # –ò—â–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è
    m = re.search(r"(?:–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å|–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π)\s*[:\|]\s*(.+?)(?:\n|\|)", clean)
    if m:
        result["executor"] = m.group(1).strip()

    # –ò—â–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—ë—Ä–∞
    m = re.search(r"(?:–ö–æ–Ω—Ç—Ä–æ–ª—ë—Ä|–ö–æ–Ω—Ç—Ä–æ–ª–µ—Ä|–ù–∞ –∫–æ–Ω—Ç—Ä–æ–ª–µ —É)\s*[:\|]\s*(.+?)(?:\n|\|)", clean)
    if m:
        result["controller"] = m.group(1).strip()

    # –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
    for dt in ["—Å–ª—É–∂–µ–±–Ω–∞—è –∑–∞–ø–∏—Å–∫–∞", "–ø–∏—Å—å–º–æ", "–ø—Ä–∏–∫–∞–∑", "—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ",
                "–ø—Ä–æ—Ç–æ–∫–æ–ª", "–∞–∫—Ç", "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ", "—Å–ø—Ä–∞–≤–∫–∞", "–¥–æ–∫–ª–∞–¥–Ω–∞—è"]:
        if dt in clean.lower():
            result["document_type"] = dt
            break

    return result


def parse_email(email_data: dict) -> dict:
    """–û–±–æ–≥–∞—â–∞–µ—Ç email-–¥–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏."""
    subject = email_data.get("subject", "")
    body_text = email_data.get("body_text", "")
    body_html = email_data.get("body_html", "") or email_data.get("body_preview", "")

    # –ï—Å–ª–∏ –Ω–µ—Ç plain-text, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ HTML
    if not body_text and body_html:
        body_text = _strip_html(body_html)

    body = body_text or ""
    sender = email_data.get("sender", "")
    sender_name = email_data.get("sender_name", "")

    enriched = dict(email_data)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ä–≤–∞—Ä–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∏—Å–µ–º
    fwd = _parse_forwarded_headers(body)
    if fwd:
        enriched["is_forwarded"] = True
        enriched["forward_info"] = fwd
        if fwd.get("original_sender"):
            enriched["original_sender"] = fwd["original_sender"]
        if fwd.get("original_subject"):
            enriched["original_subject"] = fwd["original_subject"]
    else:
        enriched["is_forwarded"] = False

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –°–≠–î-—Ç–∞–±–ª–∏—Ü
    is_sed = "[–°–≠–î]" in subject or "—Å—ç–¥" in subject.lower()
    if is_sed:
        sed_data = _extract_sed_table_data(body_html or body)
        enriched["sed_data"] = sed_data
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∞–≤—Ç–æ—Ä —Ä–µ–∑–æ–ª—é—Ü–∏–∏ ‚Äî —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –∏–Ω–∏—Ü–∏–∞—Ç–æ—Ä
        if sed_data.get("resolution_author"):
            enriched["resolution_author"] = sed_data["resolution_author"]

    # –î–æ–º–µ–Ω –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è (—Ä–µ–∞–ª—å–Ω—ã–π, —Å —É—á—ë—Ç–æ–º —Ñ–æ—Ä–≤–∞—Ä–¥–∞)
    real_sender = enriched.get("original_sender", sender)
    domain = real_sender.split("@")[-1] if "@" in real_sender else ""
    enriched["sender_domain"] = domain

    # –û—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    clean_body = body
    # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ñ–æ—Ä–≤–∞—Ä–¥–µ—Ä–∞
    clean_body = re.sub(
        r"–ü–µ—Ä–µ—Å–ª–∞–Ω–æ —Ñ–æ—Ä–≤–∞—Ä–¥–µ—Ä–æ–º\s+–ü–∞–ø–∫–∞:.*?–¢–µ–º–∞:.*?\n",
        "", clean_body, flags=re.DOTALL
    )

    # –ò—â–µ–º –¥–∞—Ç—ã/–¥–µ–¥–ª–∞–π–Ω—ã
    search_text = f"{subject} {clean_body}"
    enriched["extracted_deadlines"] = _extract_deadlines(search_text)

    # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ–¥–ª–∞–π–Ω –∏–∑ –°–≠–î –µ—Å–ª–∏ –µ—Å—Ç—å
    if is_sed and enriched.get("sed_data", {}).get("deadline"):
        dl = enriched["sed_data"]["deadline"]
        if dl not in enriched["extracted_deadlines"]:
            enriched["extracted_deadlines"].insert(0, dl)

    # –§–ò–û
    enriched["mentioned_people"] = _extract_people(clean_body)

    # –ù–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    enriched["document_refs"] = _extract_doc_refs(search_text)

    # –Ø–∑—ã–∫
    enriched["language"] = _detect_language(clean_body)

    # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏
    enriched["potential_tasks"] = _extract_task_hints(clean_body)

    # –ï—Å–ª–∏ –°–≠–î –∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç —Ä–µ–∑–æ–ª—é—Ü–∏–∏ ‚Äî —ç—Ç–æ —Ç–æ–∂–µ –∑–∞–¥–∞—á–∞
    if is_sed and enriched.get("sed_data", {}).get("resolution_text"):
        res_text = enriched["sed_data"]["resolution_text"]
        if res_text not in enriched["potential_tasks"]:
            enriched["potential_tasks"].insert(0, res_text)

    # –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
    clean = re.sub(r"\s+", " ", clean_body).strip()
    enriched["clean_preview"] = clean[:300]

    # –ß–∏—Å—Ç—ã–π body –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    enriched["body_clean"] = clean_body[:2000]

    return enriched


def _extract_deadlines(text: str) -> list[str]:
    """–ò—â–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—Ä–æ–∫–æ–≤ –∏ –¥–∞—Ç."""
    patterns = [
        r"–¥–æ\s+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"–Ω–µ\s+–ø–æ–∑–¥–Ω–µ–µ\s+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"–≤\s+—Å—Ä–æ–∫\s+–¥–æ\s+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"(?:–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–π\s+)?—Å—Ä–æ–∫[:\s]+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"–¥–µ–¥–ª–∞–π–Ω[:\s]+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"deadline[:\s]+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"–∏—Å–ø–æ–ª–Ω–∏—Ç—å\s+–¥–æ\s+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"–¥–æ\s+–∫–æ–Ω—Ü–∞\s+(–¥–Ω—è|–Ω–µ–¥–µ–ª–∏|–º–µ—Å—è—Ü–∞)",
        r"(—Å–µ–≥–æ–¥–Ω—è)",
        r"(–∑–∞–≤—Ç—Ä–∞)",
        r"(—Å—Ä–æ—á–Ω–æ)",
    ]
    results = []
    for pat in patterns:
        found = re.findall(pat, text, re.IGNORECASE)
        results.extend(found)
    return list(dict.fromkeys(results))  # –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞


def _extract_people(text: str) -> list[str]:
    """–ò—â–µ—Ç –§–ò–û –≤ —Ç–µ–∫—Å—Ç–µ."""
    patterns = [
        r"([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.)",           # –ò–≤–∞–Ω–æ–≤ –ò.–ò.
        r"([–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.\s+[–ê-–Ø–Å][–∞-—è—ë]+)",           # –ò.–ò. –ò–≤–∞–Ω–æ–≤
        r"([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+(?:–∏—á|–≤–Ω–∞|–≤–∏—á|–æ–≤–Ω–∞))",  # –ü–æ–ª–Ω–æ–µ –§–ò–û
    ]
    results = []
    for pat in patterns:
        found = re.findall(pat, text)
        results.extend(found)
    return list(set(results))


def _extract_doc_refs(text: str) -> list[str]:
    """–ò—â–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã: –Ω–æ–º–µ—Ä–∞, –¥–∞—Ç—ã, —Ä–µ–∑–æ–ª—é—Ü–∏–∏."""
    patterns = [
        r"(?:‚Ññ|–†–µ–≥\.?\s*‚Ññ)\s*([\w\-/]+(?:/\d{4}[\-\w]*)?)",
        r"((?:–°–ó|–í—Ö|–ò—Å—Ö)[\-]\d+[\-/\w]*)",        # –°–ó-70-00-06-2853/2026
        r"–æ—Ç\s+(\d{1,2}[\./]\d{1,2}[\./]\d{2,4})",
        r"(–ø—Ä–∏–∫–∞–∑[–∞-—è]*\s+(?:‚Ññ\s*)?[\d\-/]+)",
        r"(—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏[–∞-—è]+\s+(?:‚Ññ\s*)?[\d\-/]+)",
        r"(–ø—Ä–æ—Ç–æ–∫–æ–ª[–∞-—è]*\s+(?:‚Ññ\s*)?[\d\-/]+)",
        r"(—Ä–µ–∑–æ–ª—é—Ü–∏[–∞-—è]+\s+(?:‚Ññ\s*)?[\w\-/]+)",
    ]
    results = []
    for pat in patterns:
        found = re.findall(pat, text, re.IGNORECASE)
        results.extend(found)
    return list(set(results))


def _detect_language(text: str) -> str:
    if not text:
        return "unknown"
    rus = len(re.findall(r"[–∞-—è—ë–ê-–Ø–Å]", text))
    eng = len(re.findall(r"[a-zA-Z]", text))
    if rus > eng:
        return "ru"
    elif eng > rus:
        return "en"
    return "mixed"


def _extract_task_hints(text: str) -> list[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ—Ä–∞–∑—ã, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ –ø–æ—Ä—É—á–µ–Ω–∏—è/–∑–∞–¥–∞—á–∏."""
    config = load_config()
    task_keywords = config.get("task_keywords", [])
    hints = []
    sentences = re.split(r"[.!?\n]", text)
    for sentence in sentences:
        s = sentence.strip()
        if not s or len(s) < 10:
            continue
        for kw in task_keywords:
            if kw.lower() in s.lower():
                hints.append(s[:200])
                break
    return hints[:5]


def generate_summary(email_data: dict) -> dict:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ –ø–∏—Å—å–º–∞ –¥–ª—è owner'–∞."""
    parsed = parse_email(email_data)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—è
    sender_display = (
        parsed.get("resolution_author")
        or parsed.get("sender_name")
        or parsed.get("original_sender")
        or parsed.get("sender", "")
    )
    subject = parsed.get("original_subject") or parsed.get("subject", "(–±–µ–∑ —Ç–µ–º—ã)")

    priority_emoji = {
        "critical": "üî¥", "high": "üü†", "medium": "üü°", "low": "üü¢", "spam": "‚ö™"
    }
    p = parsed.get("priority", "medium")
    emoji = priority_emoji.get(p, "üü°")

    category_labels = {
        "sed": "–°–≠–î", "task": "–ü–æ—Ä—É—á–µ–Ω–∏–µ", "report": "–û—Ç—á—ë—Ç",
        "incident": "–ò–Ω—Ü–∏–¥–µ–Ω—Ç", "info": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "meeting": "–°–æ–≤–µ—â–∞–Ω–∏–µ",
        "external": "–í–Ω–µ—à–Ω–µ–µ", "personal": "–õ–∏—á–Ω–æ–µ", "newsletter": "–†–∞—Å—Å—ã–ª–∫–∞",
    }
    cat = category_labels.get(parsed.get("category", ""), "")

    lines = [
        f"{emoji} **{sender_display}**",
        f"üìã {subject}",
    ]
    if cat:
        lines.append(f"üè∑ {cat}")

    # –°–≠–î-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    sed_data = parsed.get("sed_data", {})
    if sed_data.get("document_number"):
        lines.append(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {sed_data['document_number']}")
    if sed_data.get("resolution_text"):
        lines.append(f"üìù –†–µ–∑–æ–ª—é—Ü–∏—è: {sed_data['resolution_text'][:150]}")
    if sed_data.get("executor"):
        lines.append(f"üë§ –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å: {sed_data['executor']}")

    if parsed.get("extracted_deadlines"):
        lines.append(f"‚è∞ –°—Ä–æ–∫–∏: {', '.join(parsed['extracted_deadlines'][:3])}")
    if parsed.get("mentioned_people"):
        lines.append(f"üë• –£–ø–æ–º—è–Ω—É—Ç—ã: {', '.join(parsed['mentioned_people'][:3])}")
    if parsed.get("document_refs"):
        lines.append(f"üìé –î–æ–∫—É–º–µ–Ω—Ç—ã: {', '.join(parsed['document_refs'][:3])}")
    if parsed.get("potential_tasks"):
        lines.append(f"üìå –í–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏: {len(parsed['potential_tasks'])}")
    if parsed.get("has_attachments"):
        att = parsed.get("attachment_names", [])
        lines.append(f"üìé –í–ª–æ–∂–µ–Ω–∏—è: {', '.join(att) if att else '–µ—Å—Ç—å'}")
        # –•–∏–Ω—Ç –¥–ª—è doc-review, –µ—Å–ª–∏ –µ—Å—Ç—å .docx
        att_paths = parsed.get("attachment_paths", [])
        docx_files = [p for p in att_paths if p.endswith((".docx", ".doc"))]
        if docx_files:
            lines.append(f"üìù doc-review: {len(docx_files)} –¥–æ–∫—É–º–µ–Ω—Ç(–æ–≤) –¥–ª—è —Ä–µ—Ü–µ–Ω–∑–∏–∏")

    preview = parsed.get("clean_preview", "")
    if preview and not sed_data.get("resolution_text"):
        lines.append(f"\n{preview[:200]}...")

    action_labels = {
        "create_task": "‚û°Ô∏è –°–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É",
        "reply": "‚úâÔ∏è –û—Ç–≤–µ—Ç–∏—Ç—å",
        "forward": "‚ÜóÔ∏è –ü–µ—Ä–µ—Å–ª–∞—Ç—å",
        "archive": "üì• –í –∞—Ä—Ö–∏–≤",
        "ignore": "‚è≠ –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å",
        "escalate": "üî∫ –≠—Å–∫–∞–ª–∏—Ä–æ–≤–∞—Ç—å",
        "delegate": "üë§ –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å",
        "monitor": "üëÅ –ù–∞ –∫–æ–Ω—Ç—Ä–æ–ª–µ",
    }
    action = parsed.get("proposed_action", "")
    if action:
        lines.append(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {action_labels.get(action, action)}")

    return {
        "summary_text": "\n".join(lines),
        "parsed": parsed,
    }


def main():
    parser = argparse.ArgumentParser(description="Email parser & enricher")
    sub = parser.add_subparsers(dest="command")

    p_parse = sub.add_parser("parse")
    p_parse.add_argument("--email", required=True)

    p_sum = sub.add_parser("summary")
    p_sum.add_argument("--email", required=True)

    p_tasks = sub.add_parser("extract_tasks")
    p_tasks.add_argument("--email", required=True)

    args = parser.parse_args()
    if not args.command:
        parser.print_help()
        sys.exit(1)

    data = json.loads(Path(args.email).read_text("utf-8"))

    if args.command == "parse":
        output_json(parse_email(data))
    elif args.command == "summary":
        output_json(generate_summary(data))
    elif args.command == "extract_tasks":
        parsed = parse_email(data)
        output_json({"tasks": parsed.get("potential_tasks", [])})


if __name__ == "__main__":
    main()
